{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains code and comments from Section 5.2 of the book [Ensemble Methods for Machine Learning](https://www.manning.com/books/ensemble-methods-for-machine-learning). Please see the book for additional details on this topic. This notebook and code are released under the [MIT license](https://github.com/gkunapuli/ensemble-methods-notebooks/blob/master/LICENSE)._\n",
    "\n",
    "## 5.2 Gradient Boosting: Gradient Descent + Boosting\n",
    "Gradient boosting combines gradient descent and boosting. Instead of computing the overall true gradient explicitly, gradient boosting aims to **approximate the true gradient** with a weak learner. \n",
    "\n",
    "Let's begin by generating a simple two-dimensional to visualize how gradient descent works step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=200, noise=0.15, random_state=13)\n",
    "y = 2 * y - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from visualization import plot_2d_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 4))\n",
    "plot_2d_data(ax, X, y, xlabel='x', ylabel='y', s=30,\n",
    "             title='An example classification problem', \n",
    "             legend=['pos', 'neg'], colormap='RdBu')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same loss function as in Sec. 5.2: the squared loss, as well as its gradient. The squared loss is defined slightly differently: it takes the true labels $\\mathbf{y}$ and predicted labels $f$. This is because, unlike the previous example, where we assumed a linear classifier, here $f$ can be any classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the definition of the loss function, which is defined over the **entire training set of $n$ training examples**:\n",
    "\\\\[\n",
    "L(w_1, w_2) = \\frac{1}{2} \\sum_{i=1}^n \\left( y_i - f(\\mathbf{x}_i) \\right)^2.\n",
    "\\\\]\n",
    "\n",
    "For a single training example $\\mathbf{x}_i$:\n",
    "\\\\[\n",
    "L(w_1, w_2; \\mathbf{x}_i) = \\frac{1}{2} \\left( y_i - f(\\mathbf{x}_i) \\right)^2.\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the **gradient** of the loss function: the gradient of $L(w_1, w_2; \\mathbf{x}_i)$ with respect to $f(\\mathbf{x}_i)$:\n",
    "\\\\[\n",
    "g(w_1, w_2) = \\frac{\\partial L(w_1, w_2; \\mathbf{x}_i)}{\\partial f(\\mathbf{x}_i)}\n",
    "    = -(y_i - f(\\mathbf{x}_i))\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is simply the difference between the true and predicted labels, which is also known as the **residual**. Training examples that are badly misclassified will have large functional gradients as the gap between the true and predicted labels will be large. Training examples that are correctly classified will have small functional gradients.\n",
    "\n",
    "Let's say that our classifier is $f(\\mathbf{x}_i) = 0.5$. This is a terrible classifier that can't decide if the training examples belong to the positive class ($y_i = 1$) or the negative class ($y_i=0$) as it always returns $0.5$ for all inputs.\n",
    "\n",
    "The gradients, or the residuals, can be computed for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "f = np.full((len(y), ), fill_value=0.0)\n",
    "residuals = -(y - f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For positive examples, we have that the **negative** residual $r_i = y_i - f(\\mathbf{x}_i) = 1 - 0 = +1$. This residual suggests that the classification of the positive examples can be improved by pushing the model by towards $1$. For negative examples, $r_i = y_i - f(\\mathbf{x}_i) = -1 - 0 = -1$, which suggests that the classification of the negative examples can be improved by pushing the model by away from $1$ and towards $-1$.\n",
    "\n",
    "The residual ($r_i$) is a measure of how badly a particular training example is misclassified. This is similar to the weight ($D_i$) of training examples in AdaBoost, which also represented how badly a training example was misclassified. \n",
    "\n",
    "Again, similar to AdaBoost, we will train a weak learner. In AdaBoost, we used weighted training examples $(\\mathbf{x}_i, y_i, D_i)$ to train a weak classifier. In gradient boosting, we will use training examples and residuals $(\\mathbf{x}_i, r_i)$ to train a **weak regressor**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "h = DecisionTreeRegressor(max_depth=1)\n",
    "h.fit(X, -residuals)  # Observe the negated residuals: we want to approximate the negative gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This weak regressor is an approximate gradient as it was fit to residuals, which are the **pointwise gradients**.\n",
    "\n",
    "\n",
    "### 5.2.1 Intuition: Learning with residuals\n",
    "The key component of sequential ensemble methods, such as AdaBoost and gradient boosting, is that they aim to train a new weak estimator at each iteration to fix the errors made by the weak estimator at the previous iteration. However, AdaBoost and gradient boosting train new weak estimators on poorly classified examples in rather different ways.\n",
    "\n",
    "In contrast, gradient boosting uses residuals or errors (between the true and predicted labels) to tell the base learning algorithm which training examples it should focus on in the next iteration. \n",
    "\n",
    "What exactly is a residual? For a training example, it is simply the error between the true label and the corresponding prediction. Intuitively, a correctly classified example must have a small residual and a misclassified example must have a large residual.\n",
    "\n",
    "More concretely, if a classifier $h$ makes a prediction $h(x)$ on a training example $x$, we often measure the quality of the prediction using a **loss function**. Let's say we use the squared loss\n",
    "\n",
    "$$loss(true, predicted) = \\frac{1}{2} (true - predicted)^2 = \\frac{1}{2} (y - h(x))^2.$$\n",
    "\n",
    "The resisual then, is simply the negative gradient of the loss function:\n",
    "\n",
    "$$ residual(true,predicted) = true - predicted = y-h(x).$$\n",
    "\n",
    "In the example below, we inspect gradient boosting iterations to see what these residuals (measured according to the squared loss functions) look like, and visualize the corresponding weak learners trained by gradient boosting. \n",
    "\n",
    "**NOTE**: Different loss functions lead to different residuals, since their gradients will be different. Gradient boosting can handle many such loss functions, which makes it a general approach to learning several different types of classification, regression, ranking problems and many more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from visualization import plot_2d_classifier, get_colors\n",
    "from scipy.optimize import minimize_scalar\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set up a mesh for 3d plotting\n",
    "xMin, xMax = X[:, 0].min() - 0.25, X[:, 0].max() + 0.25\n",
    "yMin, yMax = X[:, 1].min() - 0.25, X[:, 1].max() + 0.25\n",
    "xMesh, yMesh = np.meshgrid(np.arange(xMin, xMax, 0.05), \n",
    "                           np.arange(yMin, yMax, 0.05))\n",
    "\n",
    "cm = get_colors(n_colors=2, colormap='RdBu')\n",
    "views = [(36, 118), (48, -116), (11, 108)]\n",
    "\n",
    "# Initialize\n",
    "n_samples, n_features = X.shape\n",
    "n_estimators = 10\n",
    "ensemble = []\n",
    "F = np.full((n_samples, ), 0.0)  # Predictions of each training example using the ensemble\n",
    "\n",
    "for t in range(n_estimators):\n",
    "    # Fit a weak learner to the residuals, which are computed as gradient(Loss(y, F))\n",
    "    residuals = y - F\n",
    "    h = DecisionTreeRegressor(max_depth=1)\n",
    "    h.fit(X, residuals)\n",
    "\n",
    "    # ----------------------------------------------    \n",
    "    # Visualize the residuals and their fit in 3d\n",
    "    if t in [0, 1, 2]:\n",
    "        fig = plt.figure(figsize=(9, 5))\n",
    "        \n",
    "        ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "        ax.view_init(elev=views[t][0], azim=views[t][1])\n",
    "        ax.scatter(X[y<0, 0], X[y<0, 1], residuals[y<0], c=cm[0], \n",
    "                   marker='o', s=40, edgecolors='k')\n",
    "        ax.scatter(X[y>0, 0], X[y>0, 1], residuals[y>0], c=cm[1], \n",
    "                   marker='s', s=40, edgecolors='k')\n",
    "        ax.set_xlabel('$x_1$')\n",
    "        ax.set_xlim(xMin, xMax)\n",
    "        ax.set_ylabel('$x_2$')\n",
    "        ax.set_ylim(yMin, yMax)\n",
    "        ax.set_zlabel('Negative Residuals')\n",
    "        ax.set_zlim(-1, 1)\n",
    "        ax.set_title('Iteration {0}: (Negative) Sample Residuals '.format(t + 1))\n",
    "\n",
    "        ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "        zMesh = h.predict(np.c_[xMesh.ravel(), yMesh.ravel()])\n",
    "        zMesh = zMesh.reshape(xMesh.shape)\n",
    "        ax.plot_surface(xMesh, yMesh, zMesh, rstride=5, cstride=5, \n",
    "                        alpha=0.4, linewidth=0.25, edgecolors='k', cmap='RdBu')\n",
    "        ax.view_init(elev=views[t][0], azim=views[t][1])\n",
    "        ax.scatter(X[y<0, 0], X[y<0, 1], residuals[y<0], c=cm[0], \n",
    "                   marker='o', s=30, alpha=1, edgecolors='k')\n",
    "        ax.scatter(X[y>0, 0], X[y>0, 1], residuals[y>0], c=cm[1], \n",
    "                   marker='s', s=40, alpha=1, edgecolors='k')\n",
    "        ax.set_xlabel('$x_1$')\n",
    "        ax.set_xlim(xMin, xMax)\n",
    "        ax.set_ylabel('$x_2$')\n",
    "        ax.set_ylim(yMin, yMax)\n",
    "        ax.set_zlabel('Negative Residuals')\n",
    "        ax.set_zlim(-1, 1)\n",
    "        ax.set_title('Iteration {0}: Weak Learner'.format(t + 1))\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        \n",
    "    # ----------------------------------------------   \n",
    "\n",
    "    # Compute a step length that produces the best improvement in the loss\n",
    "    hreg = h.predict(X)\n",
    "    loss = lambda a: np.linalg.norm(y - (F + a * hreg))**2\n",
    "    step = minimize_scalar(loss, method='golden')\n",
    "    a = step.x\n",
    "      \n",
    "    # Compute the error of the new classifier\n",
    "    F += a * hreg\n",
    "    \n",
    "    # Update the ensemble\n",
    "    ensemble.append((a, h))\n",
    "    \n",
    "    # ----------------------------------------------   \n",
    "    # Plot the classification of the weak learner and the overall ensemble\n",
    "    h_as_classifier = lambda x: 2 * (h.predict(x) >= 0).astype(int) - 1\n",
    "    F_as_classifier = lambda x: 2 * (np.sum([a * h.predict(x) for (a, h) in ensemble], axis=0) >= 0).astype(int) - 1\n",
    "     \n",
    "    hpred = h_as_classifier(X)\n",
    "    herr = 1 - accuracy_score(y, hpred)\n",
    "        \n",
    "    Fpred = F_as_classifier(X)\n",
    "    Ferr = 1 - accuracy_score(y, Fpred)\n",
    "    \n",
    "    if t in [0, 1, 2, 3, 4, 5]: \n",
    "        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(9, 5))\n",
    "        plot_2d_classifier(ax[0], X, y, predict_function=h_as_classifier, s=40,\n",
    "                           alpha=0.25, xlabel=None, ylabel=None, colormap='RdBu',\n",
    "                           title='Iteration {0}: Weak Learner (err={1:4.2f}%)'.format(t + 1, herr*100))\n",
    "\n",
    "        plot_2d_classifier(ax[1], X, y, predict_function=F_as_classifier, s=40,\n",
    "                           alpha=0.25, xlabel=None, ylabel=None, colormap='RdBu', \n",
    "                           title='Iteration {0}: GB Ensemble (err={1:4.2f}%)'.format(t + 1, Ferr*100))    \n",
    "        fig.tight_layout()\n",
    "                \n",
    "    elif t == 9:\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        plot_2d_classifier(ax, X, y, predict_function=F_as_classifier, s=40,\n",
    "                           alpha=0.25, xlabel=None, ylabel=None, colormap='RdBu', \n",
    "                           title='Iteration {0}: GB Ensemble (err={1:4.2f}%)'.format(t + 1, Ferr*100))    \n",
    "\n",
    "        fig.tight_layout()\n",
    "    # ----------------------------------------------   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Implementing Gradient Boosting\n",
    "\n",
    "As before, we will put our intuition to practice by implementing our own version of gradient boosting. The basic algorithm can be outlined with the following pseudocode:\n",
    "\n",
    "``Initialize the ensemble: F=f_0``, some constant value\n",
    "\n",
    "``for t = 1 to T:``\n",
    "   1. compute the residuals for each example, $r_i^t=-∂L/∂F(x_i)$ \n",
    "   2. fit a weak decision tree regressor $h_t (x)$ using the training set $(x_i,r_i )_(i=1)^n$\n",
    "   3. compute the step length ($\\alpha_t$) using line search \n",
    "   4. update the model: $F = F + \\alpha_t⋅ h_t (x)$\n",
    "   \n",
    "**Listing 5.2**: Gradient Boosting for the squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gradient_boosting(X, y, n_estimators=10):\n",
    "     # Initialize\n",
    "    n_samples, n_features = X.shape\n",
    "    n_estimators = 10\n",
    "    estimators = []\n",
    "    F = np.full((n_samples, ), 0.0)  # Predictions of each training example using the ensemble\n",
    "    \n",
    "    for t in range(n_estimators):\n",
    "        # Fit a weak learner to the residuals, which are computed as gradient(Loss(y, F))\n",
    "        residuals = y - F\n",
    "        h = DecisionTreeRegressor(max_depth=1)\n",
    "        h.fit(X, residuals)\n",
    "    \n",
    "        # Compute a step length that produces the best improvement in the loss\n",
    "        hreg = h.predict(X)\n",
    "        loss = lambda a: np.linalg.norm(y - (F + a * hreg))**2\n",
    "        step = minimize_scalar(loss, method='golden')\n",
    "        a = step.x\n",
    "\n",
    "        # Update the ensemble predictions\n",
    "        F += a * hreg\n",
    "\n",
    "        # Update the ensemble\n",
    "        estimators.append((a, h))\n",
    "    \n",
    "    return estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is learned, we can make predictions as with the AdaBoost ensemble. Note that, just like our AdaBoost implementation previously, this model returns predictions of -1/1 rather than 0/1.\n",
    "\n",
    "**Listing 5.3**: Predictions using gradient boosted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gradient_boosting(X, estimators):\n",
    "    pred = np.zeros((X.shape[0], ))\n",
    "\n",
    "    for a, h in estimators:\n",
    "        pred += a * h.predict(X)\n",
    "\n",
    "    y = np.sign(pred)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.25, random_state=11)\n",
    "\n",
    "estimators = fit_gradient_boosting(Xtrn, ytrn)\n",
    "ypred = predict_gradient_boosting(Xtst, estimators)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "tst_err = 1 - accuracy_score(ytst, ypred)\n",
    "tst_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the performance of AdaBoost as the number of base estimators increases in the figure below. As we add more and more weak learners into the mix, the overall ensemble is increasingly boosted into a stronger, more complex and more nonlinear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "n_estimators = 20\n",
    "estimators = []                  # Initialize an empty ensemble  \n",
    "F = np.full((n_samples, ), 0.0)  # Predictions of each training example using the ensemble\n",
    "                                  \n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))\n",
    "subplot_id = -1\n",
    "for t in range(n_estimators):\n",
    "    # Fit a weak learner to the residuals, which are computed as gradient(Loss(y, F))\n",
    "    residuals = y - F\n",
    "    h = DecisionTreeRegressor(max_depth=1)\n",
    "    h.fit(X, residuals)\n",
    "\n",
    "    # Compute a step length that produces the best improvement in the loss\n",
    "    hreg = h.predict(X)\n",
    "    loss = lambda a: np.linalg.norm(y - (F + a * hreg))**2\n",
    "    step = minimize_scalar(loss, method='golden')\n",
    "    a = step.x\n",
    "\n",
    "    # Update the ensemble predictions\n",
    "    F += a * hreg\n",
    "\n",
    "    # Update the ensemble\n",
    "    estimators.append((a, h))\n",
    "\n",
    "    # Plot the ensemble\n",
    "    if t in [0, 1, 4, 8, 10, 19]:\n",
    "        subplot_id += 1\n",
    "        r, c = np.divmod(subplot_id, 3)\n",
    "        err = (1 - accuracy_score(y, predict_gradient_boosting(X, estimators))) * 100\n",
    "\n",
    "        title = 'Iteration {0}: err = {1:4.2f}%'.format(t + 1, err)\n",
    "        plot_2d_classifier(ax[r, c], X, y, \n",
    "                           predict_function=predict_gradient_boosting, predict_args=estimators,\n",
    "                           alpha=0.25, xlabel=None, ylabel=None, boundary_level=[0.0],\n",
    "                           title=title, colormap='RdBu', s=40)\n",
    "        ax[r, c].set_xticks([])\n",
    "        ax[r, c].set_yticks([])\n",
    "        \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.2.3\tGradient Boosting with scikit-learn\n",
    "\n",
    "``scikit-learn``’s [``GradientBoostingClassifier``](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) provides additional functionality including support for multi-class classification, as well as other base learning algorithms beyond decision trees. \n",
    "\n",
    "#### Standard gradient boosting\n",
    "The most important choice for ``GradientBoostingClassifier`` is the type of loss function. In our own hands-on implementation, we used the least-squares loss. ``GradientBoostingClassifier``, however, supports only the ``logistic`` and ``exponential`` loss functions. \n",
    "\n",
    "There are two other important arguments that the ``GradientBoostingClassifier`` takes (these are similar to the [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)):\n",
    "* ``n_estimators``, the number of weak learners that will be trained sequentially by AdaBoost, and\n",
    "* ``learning_rate``, an additional parameter that progressively shrinks the contribution of each successive weak learner\n",
    "\n",
    "Unlike ``AdaBoostClassifier`` which supports several different types of base estimators, ``GradientBoostingClassifier`` is a tree-based ensemble and only uses decision trees as base estimators. We can control the depth of the base estimators with the ``max_depth`` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "Xtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "ensemble = GradientBoostingClassifier(max_depth=1, \n",
    "                                      n_estimators=20, learning_rate=0.75)\n",
    "ensemble.fit(Xtrn, ytrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = ensemble.predict(Xtst)\n",
    "err = 1 - accuracy_score(ytst, ypred)\n",
    "print(err)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
